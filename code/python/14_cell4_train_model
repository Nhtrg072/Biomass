# =========================================================================
# HUẤN LUYỆN MÔ HÌNH DỰ BÁO SINH KHỐI 
# =========================================================================
from scipy.spatial import cKDTree

# Cấu hình
PERCENTILE_TO_REMOVE = 20
YEAR_TO_CALCULATE = 5
TRAIN_CONFIG = {'n_boost_round': 300, 'early_stopping': 50}

EXPORT_PATH = Path("/content/drive/MyDrive/data/Biomass_all_Model_Training")

plt.rcParams.update({
    'font.family': 'serif',
    'font.serif': ['Times New Roman'],
    'font.size': 12
})

# Tải dữ liệu
df_all = pd.read_csv(EXPORT_PATH / f'train_filtered_for_{YEAR_TO_CALCULATE}_years_{PERCENTILE_TO_REMOVE}.csv')
feature_info = joblib.load(EXPORT_PATH / f'filter_metadata_final_for_{YEAR_TO_CALCULATE}_years.joblib')
best_params = joblib.load(EXPORT_PATH / f'best_optuna_params_for_{YEAR_TO_CALCULATE}_years.joblib')

FEATURE_COLUMNS = ['agbd_predict', 'dem', 'slope', 'Dist_Rivers', 'Dist_Urban', 'rainfall', 'temperature']
TARGET_COLUMN = feature_info['target_variable']

X = df_all[FEATURE_COLUMNS]
y = df_all[TARGET_COLUMN]

print(f"Data: {len(X):,} samples | Features: {len(FEATURE_COLUMNS)}")

# Chia dữ liệu
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42)

print(f"Train: {len(X_train):,} | Valid: {len(X_valid):,} | Test: {len(X_test):,}")

# Train model
params_final = {
    'objective': 'regression',
    'metric': 'rmse',
    'boosting_type': 'gbdt',
    'verbose': -1,
    'num_threads': -1,
    'seed': 42,
    **best_params
}

train_data = lgb.Dataset(X_train, label=y_train, feature_name=FEATURE_COLUMNS)
valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data, feature_name=FEATURE_COLUMNS)

print(f"\nTraining (max {TRAIN_CONFIG['n_boost_round']} rounds)...")

model = lgb.train(
    params_final, train_data,
    num_boost_round=TRAIN_CONFIG['n_boost_round'],
    valid_sets=[train_data, valid_data],
    valid_names=['train', 'valid'],
    callbacks=[
        lgb.log_evaluation(period=30),
        lgb.early_stopping(stopping_rounds=TRAIN_CONFIG['early_stopping'], verbose=True)
    ]
)

print(f"Trained: {model.best_iteration} iterations")

# Đánh giá
y_pred_train = model.predict(X_train, num_iteration=model.best_iteration)
y_pred_valid = model.predict(X_valid, num_iteration=model.best_iteration)
y_pred_test = model.predict(X_test, num_iteration=model.best_iteration)

train_r2 = r2_score(y_train, y_pred_train)
valid_r2 = r2_score(y_valid, y_pred_valid)
test_r2 = r2_score(y_test, y_pred_test)

train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))
valid_rmse = np.sqrt(mean_squared_error(y_valid, y_pred_valid))
test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

train_mae = mean_absolute_error(y_train, y_pred_train)
valid_mae = mean_absolute_error(y_valid, y_pred_valid)
test_mae = mean_absolute_error(y_test, y_pred_test)

print(f"\n{'Metric':<10} {'Train':<12} {'Valid':<12} {'Test':<12}")
print("-" * 50)
print(f"{'R²':<10} {train_r2:<12.4f} {valid_r2:<12.4f} {test_r2:<12.4f}")
print(f"{'RMSE':<10} {train_rmse:<12.4f} {valid_rmse:<12.4f} {test_rmse:<12.4f}")
print(f"{'MAE':<10} {train_mae:<12.4f} {valid_mae:<12.4f} {test_mae:<12.4f}")

# Vẽ scatter plots với density
datasets = [
    (y_train, y_pred_train, 'Train', train_r2, train_rmse),
    (y_valid, y_pred_valid, 'Valid', valid_r2, valid_rmse),
    (y_test, y_pred_test, 'Test', test_r2, test_rmse)
]

for y_true, y_pred, label, r2, rmse in datasets:
    fig, ax = plt.subplots(figsize=(8, 7))

    # Tính mật độ
    xy = np.vstack([y_true, y_pred]).T
    tree = cKDTree(xy)
    data_range = max(y_true.max() - y_true.min(), y_pred.max() - y_pred.min())
    search_radius = data_range * 0.03
    counts = tree.query_ball_point(xy, r=search_radius, return_length=True)
    counts = np.array(counts)

    idx = counts.argsort()
    x_sorted, y_sorted, c_sorted = y_true.values[idx], y_pred[idx], counts[idx]

    scatter = ax.scatter(x_sorted, y_sorted, c=c_sorted, s=20, cmap='jet', alpha=0.7, edgecolors='none')

    cbar = plt.colorbar(scatter, ax=ax)
    cbar.ax.tick_params(labelsize=11)
    cbar.set_label('Mật độ', fontsize=13, rotation=90, labelpad=10, fontweight='bold')

    max_val = max(y_true.max(), y_pred.max())
    ax.plot([0, max_val], [0, max_val], 'k--', lw=2, label='1:1 line', alpha=0.7)

    limit_max = max_val * 1.05
    ax.set_xlim(0, limit_max)
    ax.set_ylim(0, limit_max)

    stats_text = f"R²: {r2:.3f}\nRMSE: {rmse:.3f}"
    ax.text(0.05, 0.95, stats_text, transform=ax.transAxes, fontsize=12, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.9, edgecolor='#cccccc'))

    ax.set_xlabel('Giá trị thực', fontsize=14, fontweight='bold')
    ax.set_ylabel('Giá trị dự đoán', fontsize=14, fontweight='bold')
    ax.set_title(f'Biểu đồ phân tán - {label.upper()}', fontsize=15, fontweight='bold', pad=15)
    ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
    ax.legend(loc='lower right', fontsize=11, framealpha=0.9)
    ax.set_aspect('equal', adjustable='box')

    plt.tight_layout()

    scatter_filename = f'scatter_final_{label.lower()}_{TARGET_COLUMN}.png'
    scatter_path = EXPORT_PATH / scatter_filename
    plt.savefig(scatter_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close()

    print(f"Đã lưu: {scatter_filename}")

# Feature importance
importance_df = pd.DataFrame({
    'feature': model.feature_name(),
    'importance': model.feature_importance(importance_type='gain')
}).sort_values('importance', ascending=False)

print(f"\nTop 10 Features:")
print(importance_df.head(10).to_string(index=False))

# Lưu model
model_file = EXPORT_PATH / f'final_biomass_model_{YEAR_TO_CALCULATE}_years.txt'
model.save_model(str(model_file))
print(f"\nĐã lưu model: {model_file.name}")

# Lưu metadata
SEASON = 'all'
metadata = {
    'season': SEASON,
    'year_to_calculate': YEAR_TO_CALCULATE,
    'PERCENTILE_TO_REMOVE': PERCENTILE_TO_REMOVE,
    'feature_columns': FEATURE_COLUMNS,
    'target_column': TARGET_COLUMN,
    'train_r2': float(train_r2),
    'valid_r2': float(valid_r2),
    'test_r2': float(test_r2),
    'train_rmse': float(train_rmse),
    'best_iteration': int(model.best_iteration),
    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')
}

meta_file = EXPORT_PATH / f'final_model_for_{YEAR_TO_CALCULATE}_years_metadata.joblib'
joblib.dump(metadata, meta_file)
print(f"Đã lưu metadata: {meta_file.name}")

print(f"\nKết quả (Test Set): R²={test_r2:.4f}, RMSE={test_rmse:.4f}, MAE={test_mae:.4f}")
