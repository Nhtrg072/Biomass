# =========================================================================
# CẤU HÌNH
# =========================================================================

%pip install lightgbm optuna rasterio shapely tqdm geopandas fiona earthengine-api geemap xgboost -q
import os
import warnings
from google.colab import drive
import ee
import geemap
import pandas as pd
import numpy as np
import lightgbm as lgb
import optuna
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import rasterio
from rasterio.plot import show
import matplotlib.pyplot as plt
import rasterio.mask
import geopandas as gpd
from shapely.geometry import box
from tqdm.notebook import tqdm
warnings.filterwarnings('ignore')
drive.mount('/content/drive')

# =========================================================================
# KHAI BÁO DỮ LIỆU
# =========================================================================
FOLDER_PATH = '/content/drive/MyDrive/data/trainingData/'
PERCENTILE_STR = '10'
TARGET_VARIABLE = 'agbd'

FILE_NAME_RFE = f'rfe_{PERCENTILE_STR}_p90.csv'
FULL_INPUT_PATH = os.path.join(FOLDER_PATH, FILE_NAME_RFE)
N_FEATURES_TO_SELECT = 15
N_TRIALS = 100
OPTUNA_TIMEOUT = 300
TEST_SIZE = 0.2
VALIDATION_SIZE = 0.25

# Tham số cố định cho LightGBM 
FIXED_LGBM_PARAMS = {
    'objective': 'regression', 'metric': 'rmse', 'boosting_type': 'gbdt',
    'verbose': -1, 'random_state': 42
}

# =========================================================================
# KHAI BÁO CÁC HÀM 
# =========================================================================

def load_and_prepare_data(file_path, target_col):
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        return None, None

    X = df.drop(columns=[target_col])
    y = df[target_col]
    X = X.select_dtypes(exclude=['object'])
    X = X.fillna(X.mean())

    return X, y

def optimize_lgbm_params(X_train, y_train, n_trials, timeout, val_size):
    def objective(trial):
        params = FIXED_LGBM_PARAMS.copy()
        params.update({
            'num_leaves': trial.suggest_int('num_leaves', 10, 31),
            'max_depth': trial.suggest_int('max_depth', 3, 6),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),
            'reg_alpha': trial.suggest_float('reg_alpha', 1.0, 10.0),
            'reg_lambda': trial.suggest_float('reg_lambda', 1.0, 10.0),
            'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),
            'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.8),
            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 0.8),
            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        })

        X_train_fit, X_val, y_train_fit, y_val = train_test_split(
            X_train, y_train, test_size=val_size, random_state=42
        )
        model = lgb.LGBMRegressor(**params)
        model.fit(
            X_train_fit, y_train_fit,
            eval_set=[(X_val, y_val)],
            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]
        )
        y_pred = model.predict(X_val)
        return np.sqrt(mean_squared_error(y_val, y_pred))

    optuna.logging.set_verbosity(optuna.logging.ERROR)
    study = optuna.create_study(direction='minimize')
    study.optimize(objective, n_trials=n_trials, timeout=timeout)

    best_params = FIXED_LGBM_PARAMS.copy()
    best_params.update(study.best_params)
    return best_params

class SimpleRFE:
    def __init__(self, n_features_to_select, lgbm_params, val_size=0.25, step=1):
        self.n_features_to_select = n_features_to_select
        self.lgbm_params = lgbm_params
        self.val_size = val_size
        self.step = step
        self.selected_features_ = None

    def fit(self, X_train, y_train):
        current_features = list(X_train.columns)

        while len(current_features) > self.n_features_to_select:
            X_current_train = X_train[current_features]

            X_train_fit, X_val, y_train_fit, y_val = train_test_split(
                X_current_train, y_train, test_size=self.val_size, random_state=42
            )

            model = lgb.LGBMRegressor(**self.lgbm_params)
            model.fit(
                X_train_fit, y_train_fit,
                eval_set=[(X_val, y_val)],
                callbacks=[lgb.early_stopping(stopping_rounds=30, verbose=False)]
            )

            importances = pd.Series(model.feature_importances_, index=current_features)
            features_to_remove = importances.nsmallest(self.step).index.tolist()

            for feature in features_to_remove:
                if len(current_features) > self.n_features_to_select:
                    current_features.remove(feature)

        self.selected_features_ = current_features
        return self

def get_final_metrics(selected_features, X_train, X_test, y_train, y_test, lgbm_params):
    X_train_selected = X_train[selected_features]
    X_test_selected = X_test[selected_features]

    X_train_fit, X_val, y_train_fit, y_val = train_test_split(
        X_train_selected, y_train, test_size=0.2, random_state=42
    )

    final_model = lgb.LGBMRegressor(**lgbm_params)
    final_model.fit(
        X_train_fit, y_train_fit,
        eval_set=[(X_val, y_val)],
        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]
    )

    y_pred_train = final_model.predict(X_train_selected)
    y_pred_test = final_model.predict(X_test_selected)

    return {
        'train_r2': r2_score(y_train, y_pred_train),
        'test_r2': r2_score(y_test, y_pred_test),
        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test))
    }

