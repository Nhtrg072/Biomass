# Xử lý duplicates trong file CSV training data

import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Cấu hình
INPUT_FILE = f'/content/drive/MyDrive/data/trainingData/biomass_predicted_data_{pct_str}.csv'
OUTPUT_FOLDER = '/content/drive/MyDrive/data/trainingData/'

TARGET_VARIABLE = 'agbd'
PERCENTILE_VALUE = 90
COORD_COLS = ['lon', 'lat', 'longitude', 'latitude', 'x', 'y']

# Tải dữ liệu
df = pd.read_csv(INPUT_FILE)
print(f"Số cột: {df.shape[1]}")

# Xác định features và target
if TARGET_VARIABLE not in df.columns:
    raise ValueError(f"Không tìm thấy biến mục tiêu '{TARGET_VARIABLE}'")

coord_cols_present = [col for col in COORD_COLS if col in df.columns]
X_cols = [col for col in df.columns if col != TARGET_VARIABLE and col not in coord_cols_present]

print(f"Features: {len(X_cols)}, Target: {TARGET_VARIABLE}")

# Phân tích duplicates
duplicated_mask = df[X_cols].duplicated(keep=False)
n_duplicated = duplicated_mask.sum()
n_unique = len(df[X_cols].drop_duplicates())

print(f"\nTổng: {len(df):,}, Unique: {n_unique:,} ({n_unique/len(df)*100:.1f}%), Duplicate: {len(df) - n_unique:,} ({(len(df) - n_unique)/len(df)*100:.1f}%)")

# Phân tích nhóm duplicate
duplicate_groups = df[duplicated_mask].groupby(X_cols)[TARGET_VARIABLE].agg([
    'mean', 'std', 'count', 'min', 'max', lambda x: x.quantile(PERCENTILE_VALUE/100)
]).rename(columns={'<lambda_0>': f'p{PERCENTILE_VALUE}'})

duplicate_groups = duplicate_groups[duplicate_groups['count'] > 1].sort_values('count', ascending=False)

print(f"Nhóm duplicate: {len(duplicate_groups):,}, Lớn nhất: {duplicate_groups['count'].max():.0f}, TB: {duplicate_groups['count'].mean():.1f} bản sao/nhóm")

# Kiểm tra Y không nhất quán
inconsistent = duplicate_groups[duplicate_groups['std'] > 0]
print(f"Nhóm có Y không nhất quán (std > 0): {len(inconsistent):,}")

if len(inconsistent) > 0:
    print(f"\nTop 5 nhóm không nhất quán:")
    print(f"{'N':<6} {'P90 Y':<10} {'Mean Y':<10} {'Std Y':<10} {'CV%':<8}")
    print("-"*50)

    inconsistent['cv'] = (inconsistent['std'] / inconsistent['mean'] * 100)
    top_inconsistent = inconsistent.nlargest(5, 'std')

    for idx, row in top_inconsistent.iterrows():
        print(f"{int(row['count']):<6} {row[f'p{PERCENTILE_VALUE}']:<10.2f} {row['mean']:<10.2f} {row['std']:<10.2f} {row['cv']:<8.1f}")

# Áp dụng aggregate P90
df_aggregated = df.groupby(X_cols, as_index=False).agg(
    agbd_p90=(TARGET_VARIABLE, lambda x: x.quantile(PERCENTILE_VALUE/100)),
    agbd_mean=(TARGET_VARIABLE, 'mean'),
    agbd_std=(TARGET_VARIABLE, 'std'),
    n_observations=(TARGET_VARIABLE, 'count')
)

df_aggregated = df_aggregated.rename(columns={'agbd_p90': TARGET_VARIABLE})

print(f"\nGốc: {len(df):,}, Sau aggregate: {len(df_aggregated):,}, Giảm: {(1 - len(df_aggregated)/len(df))*100:.1f}%")

# Thống kê
print(f"N_observations - Mean: {df_aggregated['n_observations'].mean():.2f}, Median: {df_aggregated['n_observations'].median():.0f}, Max: {df_aggregated['n_observations'].max():.0f}, Min: {df_aggregated['n_observations'].min():.0f}")

# Phân tích độ biến thiên
df_multi_obs = df_aggregated[df_aggregated['n_observations'] > 1].copy()
if len(df_multi_obs) > 0:
    print(f"Locations có >1 obs: {len(df_multi_obs):,}, Mean std: {df_multi_obs['agbd_std'].mean():.2f}, Median std: {df_multi_obs['agbd_std'].median():.2f}")

    df_multi_obs['cv'] = df_multi_obs['agbd_std'] / df_multi_obs['agbd_mean'] * 100
    print(f"Mean CV: {df_multi_obs['cv'].mean():.1f}%")
    if df_multi_obs['cv'].mean() > 30:
        print(f"CV cao (>{df_multi_obs['cv'].mean():.0f}%) - Dữ liệu có nhiễu lớn")

# Lưu kết quả
output_filename = os.path.basename(INPUT_FILE).replace('.csv', '_cleaned_P90.csv')
output_path = os.path.join(OUTPUT_FOLDER, output_filename)
print(f"\nFile lưu: {output_filename}")




